{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install speechbrain torchaudio torch pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDgj82vjZdhL",
        "outputId": "c392b4d4-1170-424c-d6d2-d7b0c8067d2f",
        "collapsed": true
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: speechbrain in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.11/dist-packages (7.0.2)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from speechbrain) (24.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from speechbrain) (1.15.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from speechbrain) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from speechbrain) (0.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.4.26)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (1.6.1)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (6.0.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->speechbrain) (1.1.2)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.11/dist-packages (from hyperpyyaml->speechbrain) (0.18.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain) (0.2.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "JFF-2nT-ZQe4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "import pinecone\n",
        "import uuid\n",
        "from typing import List, Union"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = \"pcsk_2yzKnb_DusX4M95CU1KTjQxkZFPdYWbtFghFc7kUD2cHzpUT4hWPLmMbPgEgT5NgoX3Fib\"\n",
        "PINECONE_ENV = \"us-east-1\"\n",
        "INDEX_NAME = \"voice-biometrics\"\n",
        "EMBEDDING_DIM = 192  # Matches ECAPA-TDNN output (spkrec-ecapa-voxceleb)\n",
        "UPSERT_BATCH_SIZE = 100"
      ],
      "metadata": {
        "id": "a9SGBp7bHEyt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "source": [
        "def audio_to_embedding(audio_path):\n",
        "    if not os.path.exists(audio_path):\n",
        "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
        "\n",
        "    try:\n",
        "        # Load the pretrained model\n",
        "        model = EncoderClassifier.from_hparams(\n",
        "            source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "            savedir=\"pretrained_models/spkrec-ecapa-voxceleb\"\n",
        "        )\n",
        "\n",
        "        # Load audio file\n",
        "        waveform, sample_rate = torchaudio.load(audio_path)\n",
        "\n",
        "        # Convert to mono if stereo\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Resample to 16kHz if necessary\n",
        "        if sample_rate != 16000:\n",
        "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Pad audio if shorter than 1 second\n",
        "        min_length = 16000\n",
        "        if waveform.shape[1] < min_length:\n",
        "            padding = min_length - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "\n",
        "        # Generate embedding\n",
        "        embedding = model.encode_batch(waveform)\n",
        "\n",
        "        return embedding.squeeze().numpy()\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error processing audio: {str(e)}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "z-iiZYnGfNV_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_embedding(embedding, file_name=\"\"):\n",
        "    print(f\"\\n=== TEST: {file_name} ===\")\n",
        "    print(f\"Type: {type(embedding)}\")\n",
        "    print(f\"Shape: {embedding.shape}\")\n",
        "    print(f\"Min: {embedding.min():.4f}, Max: {embedding.max():.4f}, Mean: {embedding.mean():.4f}, Std: {embedding.std():.4f}\")\n",
        "    print(\"First 5:\", embedding[:5], \"... Last 5:\", embedding[-5:])\n",
        "\n",
        "    if isinstance(embedding, np.ndarray) and embedding.ndim == 1 and embedding.shape[0] > 0:\n",
        "        print(\"âœ“ Valid embedding\")\n",
        "    else:\n",
        "        print(\"âœ— Invalid embedding\")"
      ],
      "metadata": {
        "id": "NpejpziRvqsn"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_pinecone():\n",
        "    # Initialize Pinecone client\n",
        "    pc = pinecone.Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
        "\n",
        "    # Check existing indexes\n",
        "    existing_indexes = pc.list_indexes().names()\n",
        "    if INDEX_NAME in existing_indexes:\n",
        "        index_description = pc.describe_index(INDEX_NAME)\n",
        "        if index_description.dimension != EMBEDDING_DIM:\n",
        "            print(f\"Index '{INDEX_NAME}' exists with dimension {index_description.dimension}, but {EMBEDDING_DIM} is required. Deleting and recreating...\")\n",
        "            pc.delete_index(INDEX_NAME)\n",
        "            print(f\"Deleted index '{INDEX_NAME}'.\")\n",
        "            pc.create_index(\n",
        "                name=INDEX_NAME,\n",
        "                dimension=EMBEDDING_DIM,\n",
        "                metric=\"cosine\",\n",
        "                spec=pinecone.PodSpec(environment=PINECONE_ENV, pod_type=\"p1.x1\")\n",
        "            )\n",
        "            print(f\"Index '{INDEX_NAME}' created with dimension {EMBEDDING_DIM}.\")\n",
        "        else:\n",
        "            print(f\"Index '{INDEX_NAME}' already exists with correct dimension ({EMBEDDING_DIM}).\")\n",
        "    else:\n",
        "        print(f\"Index '{INDEX_NAME}' not found. Creating index...\")\n",
        "        pc.create_index(\n",
        "            name=INDEX_NAME,\n",
        "            dimension=EMBEDDING_DIM,\n",
        "            metric=\"cosine\",\n",
        "            spec=pinecone.PodSpec(environment=PINECONE_ENV, pod_type=\"p1.x1\")\n",
        "        )\n",
        "        print(f\"Index '{INDEX_NAME}' created with dimension {EMBEDDING_DIM}.\")\n",
        "\n",
        "    return pc.Index(INDEX_NAME)"
      ],
      "metadata": {
        "id": "bbRQCMfPKP27"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_embeddings(vectors: List[np.ndarray], ids: List[str], metadata_list: List[dict] = None):\n",
        "    formatted = []\n",
        "    for i, vec in enumerate(vectors):\n",
        "        vec_id = ids[i]\n",
        "        vec_data = vec.tolist()\n",
        "        metadata = metadata_list[i] if metadata_list else None\n",
        "        formatted.append((vec_id, vec_data, metadata) if metadata else (vec_id, vec_data))\n",
        "    return formatted"
      ],
      "metadata": {
        "id": "JL4J2N2LKRIK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_upsert(index, data: List[Union[tuple, list]], batch_size: int = 100):\n",
        "    try:\n",
        "        for i in range(0, len(data), batch_size):\n",
        "            batch = data[i:i + batch_size]\n",
        "            index.upsert(vectors=batch)\n",
        "            print(f\"âœ… Upserted batch {i // batch_size + 1}: {len(batch)} vectors\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error upserting vectors: {str(e)}\")"
      ],
      "metadata": {
        "id": "R5OzFx8rKTua"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio_directory(directory_path):\n",
        "    embeddings = []\n",
        "    ids = []\n",
        "    metadata_list = []\n",
        "\n",
        "    # Initialize Pinecone index\n",
        "    index = init_pinecone()\n",
        "    print(\"ðŸ” Scanning for audio files...\")\n",
        "\n",
        "    # Process each audio file in the directory\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        if file_name.lower().endswith((\".wav\", \".mp3\")):\n",
        "            file_path = os.path.join(directory_path, file_name)\n",
        "\n",
        "            try:\n",
        "                embedding = audio_to_embedding(file_path)\n",
        "                test_embedding(embedding, file_name)\n",
        "\n",
        "                embeddings.append(embedding)\n",
        "                ids.append(str(uuid.uuid4()))\n",
        "                metadata_list.append({\"file_name\": file_name, \"source\": \"audio-directory\"})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Skipping {file_name}: {e}\")\n",
        "\n",
        "    # Upsert embeddings if any were generated\n",
        "    if embeddings:\n",
        "        formatted = format_embeddings(embeddings, ids, metadata_list)\n",
        "        batch_upsert(index, formatted, UPSERT_BATCH_SIZE)\n",
        "        print(f\"\\nðŸŽ‰ Finished processing {len(embeddings)} audio files.\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ No valid audio files processed.\")"
      ],
      "metadata": {
        "id": "Uo0zrRg8KW8K"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_audio_directory(\"/content/sample_data/audio_samples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af8ThZkVKYWx",
        "outputId": "af12dcb7-194b-4548-a0d9-ac12b943f534"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Using symlink found at '/content/pretrained_models/spkrec-ecapa-voxceleb/hyperparams.yaml'\n",
            "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index 'voice-biometrics' already exists with correct dimension (192).\n",
            "ðŸ” Scanning for audio files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.parameter_transfer:Collecting files (or symlinks) for pretraining in pretrained_models/spkrec-ecapa-voxceleb.\n",
            "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Using symlink found at '/content/pretrained_models/spkrec-ecapa-voxceleb/embedding_model.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"embedding_model\"] = /content/pretrained_models/spkrec-ecapa-voxceleb/embedding_model.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Using symlink found at '/content/pretrained_models/spkrec-ecapa-voxceleb/mean_var_norm_emb.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"mean_var_norm_emb\"] = /content/pretrained_models/spkrec-ecapa-voxceleb/mean_var_norm_emb.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Using symlink found at '/content/pretrained_models/spkrec-ecapa-voxceleb/classifier.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"classifier\"] = /content/pretrained_models/spkrec-ecapa-voxceleb/classifier.ckpt\n",
            "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Using symlink found at '/content/pretrained_models/spkrec-ecapa-voxceleb/label_encoder.ckpt'\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Set local path in self.paths[\"label_encoder\"] = /content/pretrained_models/spkrec-ecapa-voxceleb/label_encoder.ckpt\n",
            "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): embedding_model -> /content/pretrained_models/spkrec-ecapa-voxceleb/embedding_model.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): mean_var_norm_emb -> /content/pretrained_models/spkrec-ecapa-voxceleb/mean_var_norm_emb.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): classifier -> /content/pretrained_models/spkrec-ecapa-voxceleb/classifier.ckpt\n",
            "DEBUG:speechbrain.utils.parameter_transfer:Redirecting (loading from local path): label_encoder -> /content/pretrained_models/spkrec-ecapa-voxceleb/label_encoder.ckpt\n",
            "DEBUG:speechbrain.dataio.encoder:Loaded categorical encoding from /content/pretrained_models/spkrec-ecapa-voxceleb/label_encoder.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TEST: Uma.mp4.wav ===\n",
            "Type: <class 'numpy.ndarray'>\n",
            "Shape: (192,)\n",
            "Min: -52.4973, Max: 38.0868, Mean: -1.6393, Std: 17.8343\n",
            "First 5: [ 30.635386  -19.247034    9.84472    -6.8594832  23.763046 ] ... Last 5: [ 4.378494 19.961443 18.883963 11.850898 -9.757335]\n",
            "âœ“ Valid embedding\n",
            "âœ… Upserted batch 1: 1 vectors\n",
            "\n",
            "ðŸŽ‰ Finished processing 1 audio files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eNk1YdUbgNWV"
      }
    }
  ]
}